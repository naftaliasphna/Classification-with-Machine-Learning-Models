# -*- coding: utf-8 -*-
"""Breast Cancer Classification with Machine Learning Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rg8re2pdgNLmxkWbPpnZmKU8sL1auolB

# **Breast Cancer Classification-Machine Learning Approach**
Author : Naftalia Sophiana Purba

# Load Data
"""

# Import library
import numpy as np
import pandas as pd

# Load the dataset breast cancer from scikit-learn
from sklearn import datasets
cancer = datasets.load_breast_cancer()
X = cancer.data       # Features
Y = cancer.target     # Labels

# Convert to pandas dataframe
df_X = pd.DataFrame(X, columns=cancer.feature_names)
df_Y = pd.DataFrame(Y, columns=['label'])
df = pd.concat([df_X, df_Y], axis=1)
df.head(20)

"""# Data Exploration"""

# Import library
import matplotlib.pyplot as plt
import seaborn as sns

"""Data Information"""

df.shape

df.columns

df.label.unique()

df.info()

df.describe()

# Class distribution plot
plt.figure(figsize=(6,4))
sns.countplot(x="label", data=df, hue="label", palette="coolwarm", legend=False)
plt.xticks([0, 1], ['Benign', 'Malignant'])
plt.title("Distribution of Target Classes")
plt.xlabel("Cancer Type")
plt.ylabel("Count")
plt.show()

# Specifies the number of rows and columns for the subplot.
fig, axes = plt.subplots(nrows=4, ncols=8, figsize=(20, 10)) # 6 rows 5 columns
fig.suptitle("Feature Distribution", fontsize=40)

# Flatten axes to simplify iteration
axes = axes.flatten()

# Histogram plot for features
for i, column in enumerate(df.columns[:30]):  # Pilih 10 fitur pertama
    df[column].hist(ax=axes[i], bins=20, color='skyblue', edgecolor='black')
    axes[i].set_title(column)

# Adjust the layout so that subplots do not stick together
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

# Correlation
corr_matrix = df.corr()

# Correlation plot
plt.figure(figsize=(30, 20))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Matrix of Breast Cancer Features', fontsize=30)
plt.show()

"""# Data Preprocessing

Spliting Data
"""

# Import library
from sklearn.model_selection import train_test_split

# Split data into training and testing test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
print("Shape of training set:", X_train.shape)
print("Shape of test set:", X_test.shape)

"""Feature Scalling (Normalization/Standarization)"""

# Import library
from sklearn.preprocessing import StandardScaler

# Standarizing the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)   # Fit and transform the training data
X_test = scaler.transform(X_test)         # Transform the test data

"""# Classification & Evaluation Model"""

# Import library
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

"""# 1. Support Vector Machine (SVM)"""

# Import library
from sklearn.svm import SVC

"""Model Training"""

# Model training
svm = SVC(kernel='rbf')
svm.fit(X_train, Y_train)

# Make predictions using the test set
Y_pred_svm= svm.predict(X_test)

"""Evaluation"""

# Calculate accuracy of the model
svm_accuracy = accuracy_score(Y_test, Y_pred_svm)
print(f'Model Accuracy of SVM: {svm_accuracy:.2f} = {svm_accuracy * 100:.2f}%')

# Confusion matrix
cm = confusion_matrix(Y_test,Y_pred_svm)

# Display the confusion matix using Seaborn heatmap
plt.figure(figsize=(5, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',
            xticklabels=cancer.target_names, yticklabels=cancer.target_names)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix of SVM Model')
plt.show()

# Classification report
svm_report = classification_report(Y_test, Y_pred_svm,  output_dict=True)
svm_report_df = pd.DataFrame(svm_report).transpose()
print("Classification Report of SVM:")
display(svm_report_df)

"""## 2. Logistic Regression"""

# Import librray
from sklearn.linear_model import LogisticRegression

"""Model training"""

# Model training
logreg = LogisticRegression(solver='liblinear', random_state=42)
logreg.fit(X_train, Y_train)

# Make predictions using the test data
Y_pred_logreg = logreg.predict(X_test)

"""Evaluation"""

# Model accuracy
logreg_accuracy = accuracy_score(Y_test, Y_pred_logreg)
print(f'Model Accuracy of Logistic Regression: {logreg_accuracy:.2f} = {logreg_accuracy * 100:.2f}%')

# Confusion matrix
cm = confusion_matrix(Y_test,Y_pred_logreg)

# Display the confusion matix using Seaborn heatmap
plt.figure(figsize=(5, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',
            xticklabels=cancer.target_names, yticklabels=cancer.target_names)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix of Logistic Regression Model')
plt.show()

# Classification report
logreg_report = classification_report(Y_test, Y_pred_logreg,  output_dict=True)
logreg_report_df = pd.DataFrame(logreg_report).transpose()
print("Classification Report of Logistic Regression:")
display(logreg_report_df)

"""# 3. Random Forest"""

# Import library
from sklearn.ensemble import RandomForestClassifier

"""Model Training"""

# Model training
rf = RandomForestClassifier(n_estimators=7, random_state=42)
rf.fit(X_train, Y_train)

# Make prediction using the tes set
Y_pred_rf = rf.predict(X_test)

"""Evaluation"""

# Model accuracy
rf_accuracy = accuracy_score(Y_test, Y_pred_rf)
print(f'Model Accuracy of Random Forest: {rf_accuracy:.2f} = {rf_accuracy * 100:.2f}%')

# Confusion matrix
cm = confusion_matrix(Y_test,Y_pred_rf)

# Display the confusion matix using Seaborn heatmap
plt.figure(figsize=(5, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',
            xticklabels=cancer.target_names, yticklabels=cancer.target_names)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix of Random Forest Model')
plt.show()

# Classification report
rf_report = classification_report(Y_test, Y_pred_rf,  output_dict=True)
rf_report_df = pd.DataFrame(rf_report).transpose()
print("Classification Report of Random Forest:")
display(rf_report_df)

"""# 4.  K-Nearest Neighbors (KNN)"""

# Import library
from sklearn.neighbors import KNeighborsClassifier

"""Model training"""

# Model training
knn = KNeighborsClassifier(n_neighbors=4)
knn.fit(X_train, Y_train)

# Make prediction using the tes set
Y_pred_knn = knn.predict(X_test)

"""Evaluation"""

# Model accuracy
knn_accuracy = accuracy_score(Y_test, Y_pred_knn)
print(f'Model Accuracy of KNN: {knn_accuracy:.2f} = {knn_accuracy * 100:.2f}%')

# Confusion matrix
cm = confusion_matrix(Y_test,Y_pred_knn)

# Display the confusion matix using Seaborn heatmap
plt.figure(figsize=(5, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',
            xticklabels=cancer.target_names, yticklabels=cancer.target_names)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix of KNN')
plt.show()

# Classification report
knn_report = classification_report(Y_test, Y_pred_knn,  output_dict=True)
knn_report_df = pd.DataFrame(knn_report).transpose()
print("Classification Report of KNN:")
display(knn_report_df)

"""# 5. Naive Bayes"""

# Import library
from sklearn.naive_bayes import GaussianNB

# Model training
nb = GaussianNB()
nb.fit(X_train, Y_train)

# Make prediction using the tes set
Y_pred_nb = nb.predict(X_test)

"""Evaluation"""

# Model accuracy
nb_accuracy = accuracy_score(Y_test, Y_pred_nb)
print(f'Model Accuracy: {nb_accuracy:.2f} = {nb_accuracy * 100:.2f}%')

# Confusion matrix
cm = confusion_matrix(Y_test,Y_pred_nb)

# Display the confusion matix using Seaborn heatmap
plt.figure(figsize=(5, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',
            xticklabels=cancer.target_names, yticklabels=cancer.target_names)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix of Naive Bayes')
plt.show()

# Classification report
nb_report = classification_report(Y_test, Y_pred_nb,  output_dict=True)
nb_report_df = pd.DataFrame(nb_report).transpose()
print("Classification Report of Naive Bayes:")
display(nb_report_df)

"""# Comparison"""

# Comparison
models = ['SVM', 'Logistic Regression', 'Random Forest', 'KNN', 'Naive Bayes']
accuracies = [svm_accuracy, logreg_accuracy, rf_accuracy, knn_accuracy, nb_accuracy]

# Comparison accuracy plot
plt.figure(figsize=(10,6))
bars = plt.bar(models, accuracies, color='maroon')
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval - 0.03,  # Kurangi nilai Y agar tetap dalam batas
             f"{yval:.2f}", ha='center', va='top', color='white', fontsize=12, fontweight='bold')
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Comparison of Accuracy of Models')
plt.ylim(0, 1)
plt.show()

"""## Analysis of The Best Model
Based on the evaluation results of the five models, it was found that the Support Vector Machine has a higher accuracy compared to the other four models, namely Logistic Regression, Random Forest, KNN, and Naive Bayes. Therefore, the next step is to conduct a misclassification analysis and overfitting vs underfitting check.

For the misclassification analysis, it can be observed through the confusion matrix of the SVM obtained earlier.
"""

# Checking for underfitting or overfitting
# Checking training accuracy
Y_train_pred = logreg.predict(X_train)
train_accuracy = accuracy_score(Y_train, Y_train_pred)

# Checking testing accuracy
Y_test_pred = logreg.predict(X_test)
test_accuracy = accuracy_score(Y_test, Y_test_pred)

# Display the accuracy comparison results
print(f'Training data accuracy: {train_accuracy * 100:.2f}%')
print(f'Testing data accuracy: {test_accuracy * 100:.2f}%')

"""The model is well-generalized, as both training (98.68%) and testing (97.37%) accuracies are high and close, indicating neither overfitting nor underfitting.

# Deployment

Prediction new data (Example)
"""

# A new sample of breast cancer data
new_data = np.array([[13.2, 15.4, 85.0, 500.0, 0.1, 0.05, 1.0, 0.2, 0.1, 13.2, 0.1, 1.1, 0.0, 0.7, 1.3, 0.2, 0.3, 0.1, 0.3, 0.4, 1.0, 17.0, 30.6, 0.3, 0.5, 0.7, 1.0, 0.5, 0.6, 0.3]])  # New data point

# Scale the new data
new_data_scaled = scaler.transform(new_data)

# Predict the class of the new sample (0 = malignant, 1 = benign)
prediction = svm.predict(new_data_scaled)
print(f'Prediction: {"Malignant" if prediction[0] == 0 else "Benign"}')